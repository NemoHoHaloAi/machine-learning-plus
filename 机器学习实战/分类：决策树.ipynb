{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的主要优势就在于数据形式非常容易理解，而这也是kNN最主要的缺点之一；\n",
    "\n",
    "决策树：\n",
    "\t1. 根据某一特征进行数据集划分\n",
    "\t2. 判断划分后各个子数据集中的数据是否均为同一类型的\n",
    "\t\t是：返回节点类型\n",
    "\t\t否：使用当前子数据集创建新节点，返回步骤1处理新节点\n",
    "\t关键：\n",
    "\t\t1. 何时停止划分数据集，也是递归结束的条件ID3（ID3处理如何划分数据集，以及何时停止）；\n",
    "\t\t2. 如何决定使用那个特征划分数据集，根据香农熵，选择最大信息增益的特征划分；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的构造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分数据集的大原则是:将无序的数据变得更加有序，方法有很多也各有优劣，其中一种方法是使用信息论度量信息；\n",
    "\n",
    "划分数据集前后信息的变化称之为信息增益，知道如何计算信息增益，我们就可以获取到使得信息增益最大的那个特征作为划分数据集的特征，此处需要使用香农熵来计算信息增益；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算香农熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义，如果待分类的事务可能划分在多个分类之中，则符号 x_i 的信息定义为：\n",
    "\tl(x_i) = -log_2 p(x_i)\n",
    "\t其中p(x_i)是选择该分类的概率；\n",
    "\t为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到：\n",
    "\t-sum(p(x_i)*log_2 p(x_i)) i={1,2,3.....n} n为目标变量总类别数\n",
    "\t从公式可以看出含义是：各个类别自身的概率乘以自己的信息量，最后求和得到熵；\n",
    "    \n",
    "    可以说香农熵就是用于度量数据分类的无序程度，因此分类越多，熵会越大；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcShannonEntropy(dataSet):\n",
    "\t\t\"\"\"\n",
    "\t\t统计类别信息\n",
    "\t\t1. 获取到所有分类，创建字典\n",
    "\t\t2. 为每个字典设置其对应分类在数据集中数量\n",
    "\t\t3. 通过数量和总数计算熵\n",
    "\t\t\"\"\"\n",
    "\t\tclasses = {}\n",
    "\t\tfor d in dataSet:\n",
    "\t\t\tclasses[d[-1]] = classes.get(d[-1],0)+1 # 如果没有则设置-1+1，即0，如果有则加1\n",
    "\t\tcount = 1.*len(dataSet)\n",
    "\t\tentropy = -sum([classes[k]/count*math.log(classes[k]/count,2) for k in classes.keys()])\n",
    "\t\treturn entropy\n",
    "\n",
    "calcShannonEntropy(np.array([[1,1,'A'],[1,2,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 4, 'B': 2, 'E': 2, 'D': 1, 'F': 1}\n",
      "2.12192809489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.1219280948873624"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcShannonEntropy(np.array([[1,1,'A'],[1,2,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B'],[3,2,'A'],[3,2,'F'],[3,2,'E'],[3,2,'E'],[3,2,'D']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "熵越高,则混合的数据也越多；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(dataSet, axis, value): # 改善一下，原代码是根据value，每次返回当前数据集中对应特征值为value的数据子集，改为返回所有value的数据子集，以字典形式返回\n",
    "\t\"\"\"\n",
    "\t将数据集根据某个特征axis分割为每个特征值对应的数据子集，以dict形式返回\n",
    "\t\"\"\"\n",
    "\tdataDict = {}\n",
    "\tfor d in dataSet:\n",
    "\t\tv = dataDict.get(d[axis],[]) # 从列表中创建集合是Python语言得到列表中唯一元素值的最快方法\n",
    "\t\tv.append(list(np.append(d[:axis],d[axis+1:])))\n",
    "\t\tdataDict[d[axis]] = v\n",
    "\treturn dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': [['1', 'A'], ['2', 'B']],\n",
       " '2': [['1', 'A'], ['3', 'A']],\n",
       " '3': [['2', 'B']]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(np.array([[1,1,'A'],[1,2,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B']]), 0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择最佳特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeature2Split(dataSet):\n",
    "\t\"\"\"\n",
    "\t选择将数据集划分后总熵最小的那个特征以及对应的熵值，数据子集的dict形式返回\n",
    "\t\"\"\"\n",
    "\taxiss = len(dataSet[0])-1 # 去掉最后一个目标变量，否则肯定是该变量最符合要求，按照目标变量划分数据再用目标变量来评估xD\n",
    "\tminEntropy = 99999999 # 最初的无序度量值,用于与划分完之后的数据集计算的熵值进行比较\n",
    "\tminAxis = -1\n",
    "\tminDataDict = None\n",
    "\tfor axis in range(axiss):\n",
    "\t\tdataDict = splitDataSet(dataSet, axis, None)\n",
    "\t\tsumEntropy = sum([calcShannonEntropy(dataDict[k]) for k in dataDict.keys()])\n",
    "\t\tif sumEntropy < minEntropy:\n",
    "\t\t\tminEntropy = sumEntropy\n",
    "\t\t\tminAxis = axis\n",
    "\t\t\tminDataDict = dataDict\n",
    "\tprint 'min entropy:'+str(minEntropy)\n",
    "\tprint 'min axis:'+str(minAxis)\n",
    "\treturn minEntropy, minAxis, minDataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min entropy:0.0\n",
      "min axis:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " 1,\n",
       " {'1': [['1', 'A'], ['2', 'A']],\n",
       "  '2': [['1', 'B'], ['3', 'B']],\n",
       "  '3': [['2', 'A']]})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chooseBestFeature2Split(np.array([[1,1,'A'],[1,2,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 递归构建决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递归结束的条件是:程序**遍历完**所有划分数据集的属性,或者每个分支下的所有实例都具有\n",
    "**相同**的分类。如果所有实例具有相同的分类,则得到一个叶子节点或者终止块。任何到达叶子节\n",
    "点的数据必然属于叶子节点的分类。\n",
    "\n",
    "如果数据集已经处理了所有属性,但是类标签依然不是唯一\n",
    "的,此时我们需要决定如何定义该叶子节点,在这种情况下,我们通常会采用**多数表决**的方法决\n",
    "定该叶子节点的分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def majorityCnt(classList):\n",
    "\t\"\"\"\n",
    "\t计算最多的分类并返回，用于处理在所有特征都使用完之后数据子集中依然存在多个分类的case\n",
    "\t\"\"\"\n",
    "\tclassDict = {}\n",
    "\tfor c in classList:\n",
    "\t\tclassDict[c] = classDict.get(c,0)+1\n",
    "\treturn sorted(classDict.items(),key = lambda x:x[1],reverse = True)[0][0]\n",
    "\n",
    "majorityCnt(['a','b','b','a','a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min entropy:0.918295834054\n",
      "min axis:0\n",
      "数据集在同一分类：B\n",
      "数据集在同一分类：B\n",
      "min entropy:1.0\n",
      "min axis:0\n",
      "投票表决：A\n",
      "数据集在同一分类：A\n"
     ]
    }
   ],
   "source": [
    "def createTree(dataSet, labels):\n",
    "    \"\"\"\n",
    "    递归构建决策树\n",
    "    结束条件：\n",
    "        1. 当前数据集均为同一分类，此时熵为0；\n",
    "        2. 当前数据集已经没有特征可以用于划分；\n",
    "    返回：\n",
    "        1. 返回分类标签\n",
    "    \"\"\"\n",
    "    if calcShannonEntropy(dataSet) == 0:\n",
    "        print '数据集在同一分类：'+str(labels[0])\n",
    "        return labels[0]\n",
    "    elif len(dataSet[0])==1:\n",
    "        print '投票表决：'+str(majorityCnt(labels))\n",
    "        return majorityCnt(labels)\n",
    "    else:\n",
    "        _,_,dataDict = chooseBestFeature2Split(dataSet)\n",
    "        for k in dataDict.keys():\n",
    "            createTree(dataDict[k], [i[-1] for i in dataDict[k]])\n",
    "\n",
    "createTree(np.array([[1,1,'B'],[2,1,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B']]), ['A','B','A','A','B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min entropy:0.918295834054\n",
      "min axis:0\n",
      "min entropy:1.0\n",
      "min axis:0\n",
      "{'feature 1': {'1': '\\xe6\\x95\\xb0\\xe6\\x8d\\xae\\xe9\\x9b\\x86\\xe5\\x9c\\xa8\\xe5\\x90\\x8c\\xe4\\xb8\\x80\\xe5\\x88\\x86\\xe7\\xb1\\xbb\\xef\\xbc\\x9aB', '3': '\\xe6\\x95\\xb0\\xe6\\x8d\\xae\\xe9\\x9b\\x86\\xe5\\x9c\\xa8\\xe5\\x90\\x8c\\xe4\\xb8\\x80\\xe5\\x88\\x86\\xe7\\xb1\\xbb\\xef\\xbc\\x9aB', '2': {'feature 2': {'1': '\\xe6\\x8a\\x95\\xe7\\xa5\\xa8\\xe8\\xa1\\xa8\\xe5\\x86\\xb3\\xef\\xbc\\x9aA', '3': '\\xe6\\x95\\xb0\\xe6\\x8d\\xae\\xe9\\x9b\\x86\\xe5\\x9c\\xa8\\xe5\\x90\\x8c\\xe4\\xb8\\x80\\xe5\\x88\\x86\\xe7\\xb1\\xbb\\xef\\xbc\\x9aA'}}}}\n"
     ]
    }
   ],
   "source": [
    "def createTree(dataSet, labels):\n",
    "\t\"\"\"\n",
    "\t递归构建决策树\n",
    "\t结束条件：\n",
    "\t\t1. 当前数据集均为同一分类，此时熵为0；\n",
    "\t\t2. 当前数据集已经没有特征可以用于划分；\n",
    "\t返回：\n",
    "\t\t1. 返回分类标签\n",
    "\t\"\"\"\n",
    "\tif calcShannonEntropy(dataSet) == 0:\n",
    "\t\treturn '数据集在同一分类：'+str(dataSet[0][-1])\n",
    "\t\t#return dataSet[0][-1]\n",
    "\telif len(dataSet[0])==1:\n",
    "\t\treturn '投票表决：'+str(majorityCnt(list(np.array(dataSet)[:,-1])))\n",
    "\t\t#return majorityCnt(list(np.array(dataSet)[:,-1]))\n",
    "\t\n",
    "\t_,bestFeat,dataDict = chooseBestFeature2Split(dataSet)\n",
    "\tbestFeatLabel = labels[bestFeat]\n",
    "\tdel labels[bestFeat]\n",
    "\tmyTree = {bestFeatLabel:{}}\n",
    "\tfor k in dataDict.keys():\n",
    "\t\ttmpLabels = labels[:]\n",
    "\t\tmyTree[bestFeatLabel][k] = createTree(dataDict[k], tmpLabels)#[i[-1] for i in dataDict[k]])\n",
    "\treturn myTree\n",
    "\t\t\t\n",
    "myTree = createTree(np.array([[1,1,'B'],[2,1,'B'],[2,1,'A'],[2,3,'A'],[3,2,'B']]), ['feature 1','feature 2','target'])\n",
    "print myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
